Borko define processamento de língua natural como sendo a manipulação (codificação) de uma língua com propósitos específicos como comunicação, tradução, armazenamento e recuperação de informações. O processamento automático de línguas seria uma forma de facilitar a manipulação destas utilizando métodos computacionais. Imaginava-se que as tarefas que poderiam ser melhoradas através de sua execução automática seriam: a tradução de uma língua em outra, o armazenamento e recuperação de informações, e o projeto de um autômato inteligente capaz de responder perguntas. 
Borko dizia que o processamento automático da língua nunca seria perfeito, mas que poderia ser de alta qualidade. E que apesar da língua ser indisciplinada, ingovernável, havia duas razões para que os pesquisadores continuassem desenvolvendo trabalhos em processamento automático de línguas:
- Uma razão relacionada a própria natureza humana: pesquisadores gostam de desafios, têm sede de novas descobertas;
- Uma razão prática: caso não consigamos desenvolver métodos mais eficientes de comunicação para compartilhar idéias, o progresso humano seria inibido; dada a importância da troca de informações entre pesquisadores, com a explosão de informações (que começou com a intensificação das pesquisas na guerra fria), gerou-se um grande volume de informações, surgindo a necessidade de se ter métodos que auxiliassem o pesquisador a ter acesso à literatura até mesmo para que não houvesse duplicação de trabalhos.
Nos últimos anos, com o avanço cada vez mais rápido dos computadores e da tecnologia relacionada, esta explosão de informações está cada vez mais intensa. Diariamente, trilhões de unidades de informação circulam pelo mundo. Graças a este avanço, textos em formato eletrônico são facilmente obtidos - textos clássicos de grandes escritores, textos de jornais, publicações científicas, etc. - com milhões de palavras e estruturas lingüísticas das mais variadas.
Esta grande variedade de textos fez com que o interesse por métodos empíricos de análise da língua ressurgisse entre o final da década de 80 e o início da década de 90, possibilitando o aumento do número de trabalhos na área de lingüística de corpus1, visto que, para fins de análise lingüística é necessário o uso de textos representativos da língua, dialetos ou subconjuntos da língua. Surgiram, com este renascimento dos métodos empíricos e estatísticos, vários corpora grandes, como, por exemplo, o Birmingham Corpus, resultados de esforços como: Association for Computational Linguistics Data Collection Initiative (ACL/DCI), European Corpus Initiative (ECI), British National Corpus (BNC), Linguistic Data Consortium (LDC) Consortium for Lexical Research (CLR), Electronic Dictionary Research (EDR) e Text Enconding Initiative (TEI).
A pesquisa lingüística abrange uma variedade de níveis de análise. Um exemplo típico de ferramenta de análise seria o concordanceador. Um concordanceador é um programa que recupera todas as ocorrências de uma determinada cadeia de caracteres em um corpus e as listam, permitindo inclusive que estas listas sejam manipuladas. Contudo, certos tipos de análise não podem ser obtidos apenas através da grafia das palavras sem a utilização de outras características das palavras em questão, como por exemplo, informações de natureza gramatical.
Porém, para extrair o máximo de informações de um corpus através dessas ferramentas, é necessário fornecer, como entrada para elas, um corpus já etiquetado com marcas chamadas, no inglês, de part-of-speech tags. Tais marcas ou etiquetas são, principalmente, as categorias gramaticais (morfossintáticas) das palavras do corpus. Corpora etiquetados são importantes para a construção de modelos estatísticos de gramáticas para a língua escrita formal e coloquial, desenvolvimento de teorias formais sobre as diferentes gramáticas da língua escrita e falada, investigação de fenômenos prosódicos na fala e avaliação de modelos de análise sintática. Existem vários esforços de pesquisas para marcar grandes corpora com informação lingüística, incluindo categorias gramaticais e estruturas sintáticas, por exemplo, o Penn Treebank e o British National Corpus.
As ferramentas utilizadas na etiquetagem automática de corpus são os etiquetadores (taggers). A etiquetagem automática é uma tarefa básica, bem conhecida e bastante explorada em Processamento de Línguas Naturais (PLN). É muito utilizada em diversas aplicações de PLN, como extração e recuperação de informações, por exemplo, na classificação de documentos em sites de busca da internet. Os etiquetadores para a língua inglesa atingiram um estado da arte entre 95-99% de precisão geral, visto que, independente da abordagem para etiquetagem escolhida alguns casos acabam não sendo tratados, por exemplo, por dependerem de informações semânticas, o que impõe um limite à precisão geral. Sempre haverá casos que não serão tratados, mesmo que tenhamos um lingüista genial para elaborar regras ou um etiquetador perfeito, isto porque não é possível construir um corpus que inclua todas as enunciações de uma língua dada ou subconjunto de uma língua, exceto para algumas línguas mortas, em que a quantidade de textos disponíveis é limitada.
Com a geração de léxicos a partir de corpus de treinamento, a etiquetagem de textos do inglês vem se tornando cada vez mais fácil - cerca de 90% da precisão geral de um etiquetador é efeito do simples uso do léxico na etiquetagem (probabilidades léxicas) e, cerca de 50% dos 10% restantes pode ser resolvido por modelos simples de n-gramas. Mas ainda existem 50% dos 10% restantes que fazem com que a etiquetagem não seja uma tarefa trivial. Estes 50% são resultado, por exemplo, dos problemas:
- Ambigüidade léxica que não pode ser resolvida pelo contexto - existe a ambigüidade mas o contexto em que as palavras ambíguas aparecem é o mesmo. Por exemplo: "Ele chegou rápido" (advérbio) e "Ele chegou sério" (adjetivo);
- Tamanho de contexto inadequado - o contexto formado pelo período em que a palavra foco se encontra não é suficiente para resolver a ambigüidade. Por exemplo: "Quando o canto nada importa, a vida é triste" (canto é substantivo) e "O hino nacional é fundamental. Quando o canto nada importa" (canto é verbo). O primeiro período não constitui um problema para a etiquetagem, mas sim o terceiro, pois a ambigüidade só pode ser resolvida quando se conhece o segundo período;
- Palavras desconhecidas - palavras que não fazem parte do léxico, ou que não aparecem no léxico com a etiqueta correta para aquele contexto, ou que foram escritas de forma incorreta;
- Estruturas desconhecidas - estruturas que rompem com a estrutura comumente utilizada na formação de períodos e que não tenham aparecido no corpus de treinamento. Aparecem, por exemplo, em textos literários, naqueles que usam ordem indireta ou distanciam o objeto do verbo. Um exemplo que aparece em nosso corpus é o período: "Se a poder de estacas e diques o holandês extraiu_VTI de um brejo salgado a Holanda, essa jóia do esforço, é que ali nada o favorecia". Neste exemplo, todos os etiquetadores utilizados neste trabalho etiquetaram a palavra "extraiu" como verbo transitivo indireto, pois o objeto direto "a Holanda" veio depois do objeto indireto, o que usualmente não aconteceria em outros textos, como os textos jornalísticos. Já quando apresentamos o mesmo período reescrito em uma ordem mais usual - "Se o holandês extraiu a Holanda, essa jóia do esforço, de um brejo salgado a poder de estacas e diques, é que ali nada o favorecia" - todos acertaram, etiquetando o verbo "extraiu" como verbo bitransitivo.
- Sentenças Labirinto - períodos para os quais não é possível uma análise morfossintática seqüencial. Existem momentos em que se deve voltar e repensar a atribuição inicial das etiquetas, por exemplo: "Aluna precisa de matemática vence concurso".
Muito esforço foi feito na tentativa de se obter etiquetadores cada vez mais precisos para o inglês, como a etiquetagem manual de corpus volumoso, correção da etiquetagem automática também objetivando obter corpus de treinamento maior, desenvolvimento de novas técnicas supervisionadas e não supervisionadas e adaptação de técnicas utilizadas em Aprendizado de Máquina. O uso de técnicas de Aprendizado de Máquina se deve ao fato de etiquetadores poderem ser encarados como classificadores.