Redes Neurais Artificiais (RNAs) vêm sendo amplamente utilizadas em tarefas de Reconhecimento de Padrões, envolvendo aplicações como análise de crédito e reconhecimento automático de alvos. O sucesso destas tarefas está diretamente relacionado ao desempenho da RNA utilizada, o qual é analisado a partir da sua capacidade de generalização e precisão. 
Um dos modelos mais estudados em problemas de Reconhecimento de Padrões é a rede Perceptron Multi-Camadas (Multilayer Perceptron ou MLP). Este modelo utiliza originalmente o algoritmo de treinamento back-propagation, que realiza gradiente descendente no espaço de pesos. Entretanto, para apresentar um bom desempenho, a rede MLP requer uma definição do número de neurônios e a disposição dos mesmos entre as camada antes do início do treinamento. Redes muito pequenas dificilmente resolvem problemas complexos, enquanto redes muito grandes perdem a capacidade de generalização, reduzindo o desempenho para novos dados (overfitting). A definição do tamanho da rede e a disposição dos neurônios, conhecida como escolha da topologia, não é orientada por regras ou critérios baseados no problema aplicado. Esta escolha torna-se então um problema empírico, resultando em perda de tempo e esforço na busca pela topologia mais adequada para o problema abordado. 
Com o objetivo de suprir esta limitação, as Redes Neurais Construtivas (RNCs) oferecem uma abordagem atrativa para a construção incremental de topologias a partir de uma rede mínima. Uma rede mínima possui apenas camadas de entrada e saída, necessárias para a correta representação do conjunto de dados utilizado no treinamento da Rede Neural. Durante seu treinamento, uma RNC insere novos neurônios e conexões de acordo com um critério definido por seu Algoritmo Construtivo, até que uma solução satisfatória seja encontrada ou o algoritmo seja interrompido. Desta maneira, uma topologia é criada de acordo com a necessidade do problema abordado, dispensando uma busca exaustiva pela topologia mais adequada. Entre as principais motivações do estudo de Redes Neurais Construtivas, pode-se citar: 
- Flexibilidade na exploração do espaço de topologias: RNCs superam a limitação de busca por uma solução no espaço de pesos de uma topologia determinada a priori (antes do treinamento) através da extensão controlada da busca em todo o espaço de topologias da rede;
- Trocas entre medidas de desempenho: Algoritmos Construtivos diferentes permitem a troca de certas medidas de desempenho (como tempo de aprendizado) por outras (como tamanho da rede e precisão de generalização);
- Adição de conhecimento anterior: as RNCs permitem que conhecimento específico ao problema seja incorporado através da configuração inicial da rede ou pela modificação deste conhecimento utilizando novos exemplos de treinamento;
- Aprendizado contínuo: RNCs permitem que uma Rede Neural que já possui um domínio de conhecimento em sua topologia para resolver problemas simples possa ser utilizada como rede inicial no treinamento de uma nova topologia para tratar tarefas mais complexas dentro do mesmo domínio.
As Redes Neurais Construtivas consideradas neste trabalho são extensões do modelo MLP, em que a topologia é definida durante o processo de aprendizado da rede. Esta abordagem torna-se interessante quando o tempo gasto na definição da topologia é relevante. 
Cada modelo de RNC possui seu próprio critério de inserção de novos neurônios e conexões. A diferença entre estes critérios permite a construção de redes diferentes para um mesmo problema. Portanto, não é possível determinar qual modelo é mais propício para um determinado problema sem um estudo comparativo do desempenho das RNCs.