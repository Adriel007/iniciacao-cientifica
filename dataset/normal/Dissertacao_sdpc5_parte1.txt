A computação, nos últimos anos, tem sofrido profundas transformações, resultando numa grande evolução em um tempo relativamente pequeno. Processadores tornam-se mais rápidos, a capacidade da memória aumenta, redes transmitem maior volume de dados mais rapidamente, e todo esse progresso é acompanhado por uma gradual diminuição de custo, tornando a tecnologia computacional acessível e aumentando o leque de aplicações que podem ser computacionalmente resolvidas de maneira eficiente. 
Uma área de pesquisa que apresentou grande avanço junto ao progresso tecnológico foi a computação paralela. Apesar da idéia de processar informações paralelamente ser antiga, só nos últimos anos começou realmente a se desenvolver, sendo que atualmente esta área é alvo de intenso estudo dentro da comunidade acadêmica. A expansão da computação paralela está atingindo a comunidade industrial e comercial, que enxerga muitas vantagens na ótima relação custo/desempenho que esta tem a oferecer. 
Um problema relacionado à computação paralela é o alto custo de aquisição e manutenção de arquiteturas paralelas. Além disso, a compra de uma arquitetura geralmente implica na dependência do comprador ao fabricante. Uma tendência atual, neste contexto, é a utilização de sistemas distribuídos como plataformas de execução paralela, a fim de que se forneça menor custo de implantação e maior flexibilidade no processo computacional paralelo. 
A computação paralela e a computação distribuída surgiram por motivos diferentes. A necessidade de se compartilhar recursos motivou o uso de sistemas distribuídos, enquanto a busca por maior desempenho no processo computacional motivou a utilização do processamento paralelo. Atualmente, as duas áreas têm convergido, de maneira que a combinação entre os dois enfoques computacionais oferece benefícios para as duas áreas, sendo que a computação paralela utilizando sistemas distribuídos oferece flexibilidade e uma maneira eficiente de se explorar hardware interligado disponível. 
Um paradigma muito utilizado para a implementação de programas paralelos em ambientes distribuídos fracamente acoplados, como arquiteturas paralelas de memória distribuída e sistemas distribuídos, trata-se da programação via troca de mensagens. 
Dentre os ambientes de programação via troca de mensagens, destacam-se as plataformas de portabilidade, (destinados a possibilitar o transporte de programas paralelos entre plataformas computacionais distintas) das quais dois representantes merecem destaque no cenário computacional atual: o MPI e o PVM. 
O PVM destaca-se por ser considerado por muitos autores um padrão de fato para plataformas de portabilidade, enquanto o MPI é uma tentativa de padronização de direito, levada a cabo por diversas organizações mundiais. 
O objetivo principal do MPI é garantir eficiência em qualquer plataforma paralela (arquiteturas paralelas ou sistemas distribuídos), e por isso, mesmo as funções básicas de uma biblioteca de troca de mensagens, que possibilitam enviar e receber uma mensagem, possuem diversas variações. O grande problema para o usuário é entender todas essas variações, suas vantagens e as implicações de desempenho que estas possuam. 
Dentro deste contexto, este trabalho objetiva fazer um estudo a nível de estrutura e desempenho das rotinas de comunicação ponto-a-ponto do MPI sobre sistemas distribuídos baseados em uma rede de computadores pessoais (PC's) sobre o controle do sistema operacional LINUX (versão do kernel 1.3.20). 
Esta análise é feita através da utilização de benchmarks e de um exemplo de aplicação paralela, possibilitando estudar o comportamento do MPI em determinadas situações. 
Os testes foram realizados em três implementações do MPI de domínio público que executam sobre a plataforma LINUX.
Tal procedimento possibilita uma análise comparativa entre as três implementações, a fim de se determinar, por exemplo, até que ponto uma especificação centrada na eficiência pode garanti-la em qualquer implementação. 
Por fim, será analisado o comportamento das três implementações face a uma aplicação paralela real, de maneira a comparar-se os resultados obtidos com situações reais de paralelismo. 
Em alguns casos, foram incluídos resultados obtidos com o PVM, versão 3.3.10, a fim de possibilitar uma base onde possa ser avaliado o desempenho das implementações MPI em comparação com uma plataforma de portabilidade extremamente difundida. 
Entre os pontos principais que motivaram a realização deste trabalho, destacam-se: 
- As duas principais plataformas de portabilidade na atualidade, são o PVM e o MPI; o PVM está sendo explorado por algumas dissertações de mestrado dentro deste grupo de pesquisa, de maneira que é interessante estudar também o MPI, afim de que se possua um ponto de vista mais abrangente sobre plataformas de portabilidade; 
- A aceitação da computação paralela está intimamente ligada à possibilidade de portabilidade direta de programas entre sistemas heterogêneos; neste contexto, segundo alguns autores, o MPI pode tornar-se um padrão de grande importância no futuro da computação, tanto a nível acadêmico como comercial; sob esse ponto de vista, é importante estudar o comportamento de algumas de suas implementações; 
- A plataforma de hardware utilizada (rede de PC's) é muito difundida, sendo interessante um estudo do comportamento de programas paralelos nestas plataformas; além disso, todos os softwares utilizados neste trabalho são de domínio público, de maneira que uma grande parcela da comunidade computacional tem estas ferramentas ao seu alcance;
- Os primeiros contatos do autor com o MPI, antes da definição deste trabalho, mostraram que a extensa quantidade de rotinas de comunicação dificultava a sua compreensão; assim, seria útil a existência de uma dissertação onde o usuário pudesse partir de conceitos teóricos simplificados e resultados práticos, a fim de poder escolher as melhores opções para a escrita de um algoritmo paralelo. 
Segundo Blech, o futuro da computação paralela (e, portanto, também o futuro da ciência computacional) está intimamente ligado à utilização de plataformas MIMD com memória distribuída, em virtude de sua flexibilidade e facilidade de ampliação. Essas plataformas serão formadas por sistemas distribuídos interligados via tecnologias de interconexão confiáveis e rápidas. Neste contexto se insere a importância da programação via troca de mensagens (que é utilizada para a implementação de algoritmos paralelos em memória distribuída) e também das plataformas de portabilidade (principalmente o MPI), visto a importância da possibilidade de migração de programas paralelos entre diferentes plataformas.