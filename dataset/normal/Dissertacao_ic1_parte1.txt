A crescente popularização dos computadores, trazida pelas praticidades e comodidades que eles proporcionam, gera esforços sucessivos em aprimoramentos, tanto em hardware quanto em software, para que a interação homem-máquina seja cada vez mais descomplicada e transparente.
Enfocando-se, por exemplo, os processadores de textos, percebemos a grande evolução que sofreram e ainda sofrem no sentido da incorporação de ferramentas de auxílio à confecção de documentos. Estas ferramentas tendem cada vez mais a se encarregar de detalhes como a estruturação do documento e o uso correto da língua, proporcionando ao usuário uma dedicação maior ao conteúdo do documento a ser produzido.
As ferramentas de auxílio à escrita podem ser divididas em três subclasses de acordo com o estágio no processo de escrita que elas auxiliam. Quando auxiliam na geração de textos, são chamadas ferramentas de pré-processamento. Ferramentas que auxiliam no agrupamento de idéias e na composição de um texto são classificadas como ferramentas de auxílio à escrita propriamente dito. Na terceira classe aparecem as ferramentas de pós-processamento de texto. Elas contribuem para a melhoria estrutural do texto, fazendo revisões finais, como o controle da pontuação, verificações ortográficas (escrita correta das palavras), erros mecânicos de digitação (controle de abertura e fechamento de parênteses, etc.), erros gramaticais (uso de crase, concordâncias verbal e nominal, etc.) e ainda geração de dados estatísticos do texto (número e tamanho de sentenças, parágrafos, etc.).
As ferramentas de pós-processamento de texto que trabalham com textos escritos na língua inglesa já possuem recursos mais sofisticados que os tradicionais corretores ortográficos e mecânicos, tratando de correções quanto a erros gramaticais. A língua portuguesa, certamente devido ao grau de complexidade dessa tarefa, e também devido ao estágio em que se encontra sua comunidade científica nessa área, apenas recentemente tem sido alvo de pesquisas e desenvolvimento de ferramentas para revisão gramatical. Uma dessas ferramentas é produto de um projeto de pesquisa desenvolvido no ICMSC-USP, o ReGra, que está no mercado desde julho de 1995.
O revisor DTS também é uma ferramenta mais completa de revisão, abordando quatro classes principais de erros: a revisão ortográfica; a revisão de estrutura, que localiza erros como falta de pontuação, uso desbalanceado de delimitadores, uso de palavras repetidas, etc.; a revisão gramatical que trata, entre outros, de erros de concordância nominal e verbal e uso indevido de crase; e a revisão de estilo, que verifica uso correto de pronome e da partícula 'se' e também de períodos muito longos.
ReGra constitui-se de três módulos. No módulo Estatístico são geradas informações estatísticas do texto e é calculado o índice de legibilidade, isto é, um indicador de dificuldade de entendimento do texto. O Módulo Mecânico localiza erros mecânicos cometidos durante a digitação do texto, erros que não são detectados pelo corretor ortográfico, como verificação no balanceamento de delimitadores como parênteses, etc. O terceiro módulo, e o mais importante, é o Módulo Lingüístico, que trata erros gramaticais como o uso da crase, concordâncias nominal e verbal, colocação pronominal, uso de verbos 'fazer' e 'haver', uso de 'há' e 'a', etc. Os textos que servem como base na correção estão contidos em um banco de textos (corpus), e são divididos em duas classes: os revisados e corrigidos, retirados de trabalhos científicos, jornais, revistas; e os textos sem revisão ou correção, onde estão os erros mais comuns cometidos por usuários da língua.
A implementação do revisor ReGra está fundamentada no paradigma simbolista, usando regras que efetuam uma análise local, ou seja, apenas nos itens lexicais relevantes para a análise, no caso da revisão ortográfica e mecânica. Na revisão gramatical, usa a filosofia das redes de transição ATNs (Augmented Transition Networks) em dois casos: no caso das regras pontuais, que se utilizam apenas das informações lexicais relativas ao token, está voltada para a identificação dos erros mais comuns cometidos por usuários da língua. Nos casos de concordância nominal e verbal, usa também o recurso das Definite-Clause-Grammars (DCGs), construindo tabelas de ordem de preferência para o uso das regras gramaticais, pois na ausência de informações semânticas sobre o contexto, foram estabelecidos critérios de desambiguação através da freqüência de ocorrência.
O paradigma simbolista apresenta vantagens quanto à representação de sentenças com complexidade arbitrária, pois se utiliza das técnicas de parsing, que mapeiam textos de entrada em representações internas, classificando suas constituintes. Entretanto, as regras devem ser explicitamente programadas, tendo em mente exemplos específicos. Aí surgem problemas quanto à confecção das regras gramaticais do português. Devido ao fato de a gramática possuir muitas exceções, há uma necessidade de se construir um número grande de regras para se cobrirem todos os casos, o que torna o sistema cada vez mais complexo. ReGra, por exemplo, usa um conjunto com cerca de 600 regras gramaticais.
Numa outra linha de pesquisa, sob uma visão conexionista, estão surgindo muitas aplicações nesse campo. As Redes Neurais Artificiais têm se destacado com grande eficiência na resolução de problemas de Lingüística, em casos como reconhecimento de sentenças, problemas de ambigüidade, classificação de sentenças, etiquetagem sintática (atribuição de classe sintática às palavras individuais).
Por serem modelos matemáticos, com processo de aprendizado através da atualização de pesos com a apresentação sucessiva de exemplos de treinamento, as redes neurais são capazes de generalizar para reconhecer novos exemplos, adaptando-os a novas classes com sucesso. Qualquer regularidade estatística nos exemplos de treinamento é automaticamente utilizada na construção das inferências. Assim, os modelos conexionistas provêm mecanismos mais gerais para inferência, desde que se manipule corretamente as redes para cada exemplo apresentado.
E é justamente neste contexto que se insere nosso trabalho. O objetivo é fazer um estudo comparativo do uso das técnicas conexionista e simbolista, na revisão automática de erros gramaticais da língua portuguesa. Para isso, projetamos uma Rede Neural Parcialmente Recursiva e um modelo de Rede Backpropagation, que detectam erros com relação ao uso da crase, tanto em casos de presença incorreta, quanto de ausência. Não há aqui o propósito de atacar a eficiência dos métodos simbolistas tradicionais, mas, sim, o de observar o desempenho de ambos os métodos quanto ao problema determinado, visando assim uma maior integração entre eles, aproveitando suas melhores potencialidades.
Para a implementação, utilizamos dos recursos do simulador de redes neurais SNNS (Stuttgart Neural Network Software). A opção pelo uso de um software deste tipo surgiu pelo fato de não necessitarmos de mudanças quanto à construção dos modelos das redes, ou seja, as redes neurais seriam implementadas exatamente como são propostas nos modelos originais, sendo exatamente o que dispomos no simulador.